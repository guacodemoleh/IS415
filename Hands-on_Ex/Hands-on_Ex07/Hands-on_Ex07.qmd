---
title: "Hands-on Exercise 7"
author: "Victoria Grace ANN"
execute: 
  warning: false
  eval: true
  echo: true
format:  
  html: 
    code-summary: "Show the code"
    toc-depth: 4
date: "3 March, 2024"
date-modified: "last-modified"
editor: visual
---

# Overview

This hands-on exercise focuses on how a homogeneous region is delineated using geographically referenced multivariate data.

The two major analyses focused here are: - hierarchical cluster analysis; - spatially constrained cluster analysis

## Learning Outcomes

-   Convert GIS polygon data into R's simple feature data.frame using appropriate **sf** functions
-   Convert simple feature data.fram into SpatialPolygonDataFrame using **sf** package of R
-   Perform spatially constrained cluster analysis using *skater()*
-   Visualise the analysis output using **ggplot2** and **tmap** packages

# Getting Started

## The Analytical Question

It is a common practice to delineate the market or planning area into homogeneous regions by using multivariate data. For this exercise, the Shan State of Myanmar will be delineated into homogeneous regions with respect to ICT measures including radio, television, landlind phone, mobile phone, computer, and the availability of the internet at home.

# Data

1.  Myanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.

2.  Shan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.

They are downloaded from Myanmar Information Management Unit (MIMU).

## Installing and Loading Packages

The R packages needed are: - **sf** and **spdep** for spatial data hnadling - **tidyverse**, **readr**, **ggplot2** and **dplyr** for attribute data handling - **coorplot**, **ggpubr** and **heatmaply** for multivariate data visualisation and analysis - **cluster** and **ClusterGeo** for cluster analysis!

```{r}
pacman::p_load(sp,spdep, tmap, sf, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, GGally)
```

## Data Importing and Preparation

Let's import the Myanmar Township Boundary GIS data and its associated attribute table.

The Myanmar Township Boundary GIS data is in ESRI shapefile format. Thus *st_read()* is used to import the file over to our R environment.

```{r}
shan_sf <- st_read(dsn = "data/geospatial", 
                   layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>%
  select(c(2:7))
```

The imported township boundary object, *shan_sf* is saved in **simple feature data.frame** format.

To view the contents of *shan_sf*.

```{r}
shan_sf
```

To view the data types of the attributes.

```{r}
glimpse(shan_sf)
```

## Importing aspatial data

```{r}
ict <- read_csv("data/aspatial/Shan-ICT.csv")
```

To view the summary statistics.

```{r}
summary(ict)
```

-   There are 11 attributes and 55 observations.

## Deriving new variables using dplyr

The unit of measurement of the values are the number of households. Townships with more households will have a greater number of households owning radio, TV and other ICTs.

To overcome this bias, the penetration rate of each ICT variable can be derived.

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
```

Review the summary statistics of the newly derived pentration rates.

```{r}
summary(ict_derived)
```

::: callout-note
Six new fields have been added into the data frame!
:::

# Exploratory Data Analysis (EDA)

## EDA using statistical graphics

Histograms are useful to identify the distribution of data values.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

-   The data seems to be right-skewed!

Now check for any outliers using boxplot.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

-   There are 3 outliers.

Next, plot the distribution of the newly derived variables.

For instance, to check the distribution of the radio penetration rate,

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Check for outliers

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

-   There is only one outlier.

Let's check all the distributions across multiple variables.

First, create the individual histograms and store each of them into a variable.

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Then use the *ggarrange()* function of **ggpubr** to group the histograms together.

```{r}
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)
```

## EDA using choropleth map

#### Joining geospatial data with aspatial data

```{r}
#| eval: false
shan_sf <- left_join(shan_sf, 
                     ict_derived, by=c("TS_PCODE"="TS_PCODE"))
  
write_rds(shan_sf, "data/rds/shan_sf.rds")
```

::: callout-note
TS_CODE field is the common field used to perform the left-join.
:::

```{r}
shan_sf <- read_rds("data/rds/shan_sf.rds")
```

#### Preparng a choropleth map

Look at the distribution of the radio penetration rate using a choropleth map.

A quick choropleth map can be created using *qtm()* from **tmap** package.

```{r}
qtm(shan_sf, "RADIO_PR")
```

To reveal the distribution is biased to the underlying total number of households at townships, let's compare choropleth maps for total number of households and total number of households with radios.

```{r}
TT_HOUSEHOLDS.map <- tm_shape(shan_sf) + 
  tm_fill(col = "TT_HOUSEHOLDS",
          n = 5,
          style = "jenks", 
          title = "Total households") + 
  tm_borders(alpha = 0.5) 

RADIO.map <- tm_shape(shan_sf) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

-   Townships with more households positively correlate with the radio ownership rates.

Now plot the choropleth maps showing the distribution of the total number of households and radio penetration rate.

```{r}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

-   Looking at the southern region of Shan state, there are fewer households in this part of the state yet they have moderate radio penetration rates which are similar to the rates in regions with more households.

# Correlation

Since we are talking about correlation, it is also important to ensure that cluster variables are not highly correlated.

The *corrplot.mixed()* function from **corrpplot** package can be used to visualise and analyse the correlation of the input variables.

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

-   COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both.

# Hierarchy Cluster Analysis

## Extracting clustering variables

```{r}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars,10)
```

-   INTERNET_PR was removed since it is highly correlated with COMPUTER_PR

Then, change the rows by township name rather than row number for easier referencing.

```{r}
row.names(cluster_vars) <- cluster_vars$"TS.x"
head(cluster_vars,10)
```

Delete the TS.x field.

```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

## Data Standardisation

Multiple variables will be used in cluster analysis. It is not unusual when their value ranges are different. However, standardisation of the data should be performed to prevent clustering variables with larger values to cause a bias.

## Min-Max Standardisation

From **heatmaply**, , the *normalize()* function is used to standardise the clustering variables by using the Min-Max methody.

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

## Z-score Standardisation

Z-score standardisation can be performed easily by using *scale()*.

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

::: callout-warning
Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.
:::

## Visualising Standardised Clustering Variables

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

-   All the clustered variables follow a normal distribution and the previous standardisation step was performed successfully.

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

## Computing Proximity Matrix

The *dist()* function supports six distance proximity calculations: - euclidean (default) - maximum - minimum - canberra - binary - minkowski

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
```

## Computing Hierarchical Clustering

*hclust()* employs an agglomeration method to compute the cluster. The eight clustering algorithms supported ward.D, ward.D2, single, complete, average (UPGMA), mcquitty (WPGMA), median (WPGMC) and centroid (UPGMC).

To try hierarchical cluster analysis using the ward.D method,

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

Plot the tree using *plot()*.

```{r}
plot(hclust_ward, cex = 0.6)
```

## Selecting the optimal clustering algorithm

It is difficult to identify the stronger clusters, but we can do so using *agnes()* from the **cluster** package! The agglomerative coefficient can be gathered and a coefficient value closer to 1 indicates strong clustering.

Computing the agglomerative coefficients of all hierarchical clustering algorithms,

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

-   Wardâ€™s method provides the strongest clustering structure among the four methods assessed.

## Determining Optimal Clusters

The common methods to determine the optimal clusters are: - Gap Statistic Method - Elbow Method - Average Silhouette Method

### Gap Statistic Method

The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.

To compute the gap statistic, clusGap() of cluster package will be used.

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

::: callout-note
*hcut* is from the **factoextra** package.
:::

Visualise the plot using *fviz_gap_stat()* of the **factoextra** package.

```{r}
fviz_gap_stat(gap_stat)
```

The recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.

::: callout-note
The **NbClust** package provides 30 indices to determine the relevant number of clusters and proposes to users the best clustering schemes based on different results that are obtained by various combinations of clusters, distance measures and clustering methods.
:::

## Interpreting Dendrograms

Each leaf in a dendrogram corresponds to one observation. Observations similar to each other are combined into branches as shown by tracing the dendrogram from bottom to top.

The **height** of these fused branches indicates the (dis)similarity between two observations. The higher these fusions, the less similar the observations.

To draw the dendogram with a border around the selected clusters using *rect.hclust()*, the argument `border` is used to specify the border colours of the rectangles.

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, 
            border = 2:5)
```

## Visually-driven Hierarchical Clustering Analysis

Perform visually-driven hiearchical clustering analysis by using the **heatmaply** package.

Turn the data frame into a data matrix.

```{r}
shan_ict_mat <- data.matrix(shan_ict)
```

### Plotting interactive cluster heatmap using *heatmaply()*

```{r}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )
```

-   Six clusters are identfied! (look at the number of distinct branch colours)

## Mapping Formed Clusters

Use *cutree()* to derive a 6-cluster model.

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
```

The output is called groups. It is a list object.

In order to visualise the clusters, the groups object needs to be appended to the `shan_sf` simple feature object.

To form the join in three steps: 

- Convert the *groups* list object into a matrix 
- Append the *groups* matrix to `shan_sf` using *cbind()* 

- Use *rename()* from **dplyr** to rename the *as.amtrix.groups* field as `CLUSTER`

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

Have a look at where are the clusters using a choropleth map.

```{r}
qtm(shan_sf_cluster, "CLUSTER")
```

As shown above, the clusters are very fragmented and this is a limitation of using a non-spatial clustering algorithm such as hiearchical clustering.

# Spatially Constrained Clustering: SKATER approach

## Converting into SpatialPolygonsDataFrame

Convert `shan_sf` into spdf form since the SKATER function only supports spatial (**sp**) objects.

```{r}
shan_sp <- as_Spatial(shan_sf)
```

## Computing Neighbour List

From a previous exercise, apply *poly2nb()* to compute the neighbours list for all the polygons.

```{r}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

We can also view the networks between the polygons.

```{r}
#| eval: false
plot(shan_sp, 
     border=grey(.5))
plot(shan.nb, 
     coordinates(shan_sp), 
     col="blue", 
     add=TRUE)
```

The plotting area is usually determined by the characteristics of the neighbouring plots. Since the boundary map extends further than the neighbour graph, the boundary map should be plotted first.

## Computing Minimum Spanning Tree

### Calculating Edge Costs

Next, *nbcosts()* from **spdep** can compute the cost of each edge, i.e. distance between nodes. This function computes the distance using a data frame with the observation vector in each node.

To compute the cost of each edge.

```{r}
lcosts <- nbcosts(shan.nb, shan_ict)
```

Each observation provides a pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). This is the notion of a generalised weight for a spatial weights matrix.

The next step is to incorporate these costs into a weights object in the same way the inverse distance weights were calculated. This is done by converting the neighbour list to a list weights object by specifying the computed `lcost` as the weights.

```{r}
shan.w <- nb2listw(shan.nb, 
                   lcosts, 
                   style="B")
summary(shan.w)
```


## Computing minimum spanning tree

The minimum spanning tree is computed by mean of the *mstree()* from **spdep**.

```{r}
shan.mst <- mstree(shan.w)
```

```{r}
class(shan.mst)
```

```{r}
dim(shan.mst)
```
The dimension is 54 since there are a minimum of (n-1) edges to traverse all nodes.


```{r}
head(shan.mst)
```

The plot method for the Minimum Spanning Tree is a method for showing the observable number of nodes on top of the edge. 


```{r}
plot(shan_sp, border=gray(.5))
plot.mst(shan.mst, 
         coordinates(shan_sp), 
         col="blue", 
         cex.lab=0.7, 
         cex.circles=0.005, 
         add=TRUE)
```


## Computing spatially constrained clusters using SKATER method

Using SKATER,
```{r}
clust6 <- spdep::skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
```

::: {.callout-note}
### Interpreting the above code
-   the first two columns of the MST matrix are extracted

-   the data argument from Shan State's ICT information

-   the number of cuts which will lead to n+1 clusters formed
:::

```{r}
str(clust6)
```


```{r}
ccs6 <- clust6$groups
ccs6
```

To check the number of observations for each cluster,

```{r}
table(ccs6)
```

Plot out the clustered networks or pruned tree over the townships.
```{r}
plot(shan_sp, border=gray(.5))
plot(clust6, 
     coordinates(shan_sp), 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink"),
     cex.circles=0.005, 
     add=TRUE)
```


## Visualising CLusters in Choropleth Map

To plot the newly derived clusters using SKATER.

```{r}
groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster, "SP_CLUSTER")
```


For easier comparison between hierarchical clustering and spatially constrained hierarchical clustering,

```{r}
hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 

shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```


# Spatially COnstrained Clustering: ClustGeo Method

In this section, we use functions provided by ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.


## About ClustGeo package

**ClusterGeo** is designed to support the need of performing spatially constrained cluster analysis, where it provides a Ward-like hierarchical clustering algorithm called *hclustgeo()* that considers spatial or geographical constraints.

The algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, where the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.

## Ward-like hierarchical clustering: ClustGeo

Typically, *hclustgeo()* performs Ward-like hierarchical clustering like what *hclust()* does.

To perform non-spatially constrained hierarchical clustering, we only need to pass a dissimilarity matrix in the function.

```{r}
nongeo_cluster <- hclustgeo(proxmat)
plot(nongeo_cluster, cex = 0.5)
rect.hclust(nongeo_cluster, 
            k = 6, 
            border = 2:5)
```

::: {.callout-note}
The dissimilarity matrix must be an object of class *dist*, where the object is developed.
:::

### Mapping the formed clusters

```{r}
groups <- as.factor(cutree(nongeo_cluster, k=6))

shan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)

qtm(shan_sf_ngeo_cluster, "CLUSTER")
```


## Spatially Constrained Hierarchical Clustering

Spatial distance matrice can be derived by using *st_distance()*.

```{r}
dist <- st_distance(shan_sf, shan_sf)
distmat <- as.dist(dist)
```

-   Th data frame has been converted to a matrix using *as.dist()*

Net, *choicealpha()* will be used to determine a suitable value for the mixing parameter alpha.

```{r}
cr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)
```


```{r}
clustG <- hclustgeo(proxmat, distmat, alpha = 0.3)
```


```{r}
groups <- as.factor(cutree(clustG, k=6))
```


```{r}
shan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
```

```{r}
qtm(shan_sf_Gcluster, "CLUSTER")
```


# Visual Interpretation of Clusters

## Visualising individual clustering variable

```{r}
ggplot(data = shan_sf_ngeo_cluster,
       aes(x = CLUSTER, y = RADIO_PR)) +
  geom_boxplot()
```


## Multivariate Visualisation

Past studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, ggparcoord() of GGally package

```{r}
ggparcoord(data = shan_sf_ngeo_cluster, 
           columns = c(17:21), 
           scale = "globalminmax",
           alphaLines = 0.2,
           boxplot = TRUE, 
           title = "Multiple Parallel Coordinates Plots of ICT Variables by Cluster") +
  facet_grid(~ CLUSTER) + 
  theme(axis.text.x = element_text(angle = 30))
```



```{r}
shan_sf_ngeo_cluster %>% 
  st_set_geometry(NULL) %>%
  group_by(CLUSTER) %>%
  summarise(mean_RADIO_PR = mean(RADIO_PR),
            mean_TV_PR = mean(TV_PR),
            mean_LLPHONE_PR = mean(LLPHONE_PR),
            mean_MPHONE_PR = mean(MPHONE_PR),
            mean_COMPUTER_PR = mean(COMPUTER_PR))
```


